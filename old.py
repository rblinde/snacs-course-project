# -*- coding: utf-8 -*-
"""SNACS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d3p6k-W9XuI4Q_JyMBcamfXa4IOOSGV_

#TODO:

- See if the third algorithm can also be used on not-connected graphs (change it otherwise)
- Collect and save results to a file
- Plot results

# EVALUATING COMMUNITY DETECTION ALGORITHMS

####Â SNACS COURSE PROJECT 
* Robert Blinde <robertblinde@gmail.com> 
* Marco Haitink <marcohaitink@icloud.com>

In the following sections, each proposed algorithm will be tested against a ground-truth network with a well-known community structure. Algorithms will be evaluated on both **accuracy** (i.e. the ability to actually approximate the correct community structure) and **running time**. <br> 

Accuracy will be evaluated using a quality index based on Jaccard Similarity, firstly introduced by Hric et Al. in ["Community detection in networks: Structural communities versus ground truth"](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.90.062805). Running time will be calculated using the system clock imported by Python. More details on our paper.
"""

import time 
import networkx as nx
import numpy as np
from networkx.algorithms import community
from collections import defaultdict

def LabelPropagationAlgorithm(ground_truth_path, dataset_path, size):

    #initialize gorund truth
    ground_truth_set = groundTruth(ground_truth_path)
    #create network from edge list
    G = nx.read_edgelist(dataset_path)

    #list to store the result of each iteration
    iteration_accuracy = []
    iteration_time = []

    #define number of iterations
    iterations = 10
    for i in range(iterations):
        
        #call label propagation algorithm and time it
        start = time.time()
        res_lpa = community.asyn_lpa_communities(G)
        end = time.time()

        accuracy = round(percentage_of_nodes(ground_truth_set, res_lpa, size),2)
        execTime = round((end - start) * 1000, 2)

        #append results (accuracy and time) for the current iteration
        iteration_accuracy.append(accuracy)
        iteration_time.append(execTime)

    #calculate and return average accuracy and average execution time
    avg_accuracy = round(sum(iteration_accuracy) / len(iteration_accuracy), 2)
    avg_time = round(sum(iteration_time) / len(iteration_time), 2)

    f = open("results.txt","a")
    f.write("\nLabel Propagation Algorithm:\n")
    f.write("Algorithm is non-deterministic\n") 
    f.write("Results are shown for 10 iterations\n")
    if(ground_truth_path == "datasets/Karate/karate-club-labels.txt"):
        f.write("Dataset: Karate Club\n")
    elif(ground_truth_path == "datasets/email-Eu-Core/email-Eu-core-department-labels.txt"):
        f.write("Dataset: Email Eu Core\n")
    elif(ground_truth_path == "datasets/YouTube/youtube.top5000.txt"):
        f.write("Dataset: YouTube\n")
    elif(ground_truth_path == "datasets/Amazon/amazon.top5000.txt"):
        f.write("Dataset: Amazon\n")
    elif(ground_truth_path == "datasets/Football/football-comms.txt"):
        f.write("Dataset: College Football\n")

    f.write("{:<20s}{:<20s}\n".format("Accuracy[%]", "Run Time[ms]"))
    for i in range(iterations):
        f.write("{:<20f}{:<20f}".format(iteration_accuracy[i],iteration_time[i]))
        f.write("\n")
    f.write("\n{:<20s}{:<20s}\n".format("Average accuracy", "Average time"))
    f.write("{:<20f}{:<20f}\n\n".format(avg_accuracy, avg_time))
    f.close()
    
def GirvanNewmanAlgorithm(ground_truth_path, dataset_path, size):

    #initialize gorund truth
    ground_truth_set = groundTruth(ground_truth_path)
    #create network from edge list
    G = nx.read_edgelist(dataset_path)

    #list to store the result of each iteration
    iteration_accuracy = []
    iteration_time = []

    start = time.time()
    res_gn = community.girvan_newman(G)
    end = time.time()

    c = tuple(c for c in next(res_gn))

    accuracy = round(percentage_of_nodes(ground_truth_set, c, size),4)
    execTime = round((end - start) * 1000, 4)

    f = open("results.txt","a")
    f.write("\nGirvan Newman Algorithm:\n")
    if(ground_truth_path == "datasets/Karate/karate-club-labels.txt"):
        f.write("Dataset: Karate Club\n")
    elif(ground_truth_path == "datasets/email-Eu-Core/email-Eu-core-department-labels.txt"):
        f.write("Dataset: Email Eu Core\n")
    elif(ground_truth_path == "datasets/YouTube/youtube.top5000.txt"):
        f.write("Dataset: YouTube\n")
    elif(ground_truth_path == "datasets/Amazon/amazon.top5000.txt"):
        f.write("Dataset: Amazon\n")
    elif(ground_truth_path == "datasets/Football/football-comms.txt"):
        f.write("Dataset: College Football\n")

    f.write("{:<20s}{:<20s}\n".format("Accuracy[%]", "Run Time[ms]"))
    f.write("{:<20f}{:<20f}".format(accuracy, execTime))
    f.close()
    
def GreedyModularityAlgorithm(ground_truth_path, dataset_path, size): 

    #initialize gorund truth
    ground_truth_set = groundTruth(ground_truth_path)
    #create network from edge list
    G = nx.read_edgelist(dataset_path)

    #list to store the result of each iteration
    iteration_accuracy = []
    iteration_time = []

    #define number of iterations
    iterations = 10
    for i in range(iterations):
        
        #call label propagation algorithm and time it
        start = time.time()
        res_greedy = community.greedy_modularity_communities(G)
        end = time.time()

        accuracy = round(percentage_of_nodes(ground_truth_set, res_greedy, size),2)
        execTime = round((end - start) * 1000, 2)

        #append results (accuracy and time) for the current iteration
        iteration_accuracy.append(accuracy)
        iteration_time.append(execTime)

    #calculate and return average accuracy and average execution time
    avg_accuracy = round(sum(iteration_accuracy) / len(iteration_accuracy), 2)
    avg_time = round(sum(iteration_time) / len(iteration_time), 2)

    f = open("results.txt","a")
    f.write("\nGreedy Modularity Based Algorithm:\n")
    f.write("Algorithm is non-deterministic\n") 
    f.write("Results are shown for 10 iterations\n")
    if(ground_truth_path == "datasets/Karate/karate-club-labels.txt"):
        f.write("Dataset: Karate Club\n")
    elif(ground_truth_path == "datasets/email-Eu-Core/email-Eu-core-department-labels.txt"):
        f.write("Dataset: Email Eu Core\n")
    elif(ground_truth_path == "datasets/YouTube/youtube.top5000.txt"):
        f.write("Dataset: YouTube\n")
    elif(ground_truth_path == "datasets/Amazon/amazon.top5000.txt"):
        f.write("Dataset: Amazon\n")
    elif(ground_truth_path == "datasets/Football/football-comms.txt"):
        f.write("Dataset: College Football\n")

    f.write("{:<20s}{:<20s}\n".format("Accuracy[%]", "Run Time[ms]"))
    for i in range(iterations):
        f.write("{:<20f}{:<20f}".format(iteration_accuracy[i],iteration_time[i]))
        f.write("\n")
    f.write("\n{:<20s}{:<20s}\n".format("Average accuracy", "Average time"))
    f.write("{:<20f}{:<20f}\n\n".format(avg_accuracy, avg_time))
    f.close()

def percentage_of_nodes(ground_truth_set, res_set, size):
  
    #initialize and fill vector to store ground truth labels
    gt_Array = np.ones(size, dtype = int)
    pos = 0
    for s in ground_truth_set.values():
        for v in s:
            gt_Array[int(v)] = pos
        pos += 1
    
    #initialize and fill vector to store algorithm results
    alg_Array = np.ones(size, dtype = int)
    pos = 0
    for s in res_set: 
        for v in s:
            alg_Array[int(v)] = pos
        pos += 1

    #compare arrays node by node
    count = 0
    for i in range(size):
        if gt_Array[i] == alg_Array[i]:
            count += 1
    
    #labels in the YT database are not consecutives
    #this is a workaround to get the correct accuracacy measure
    #without having to re-label every node
    if(size == 1157828):
        size = 1134890

    #return percentage of correctly classified nodes
    return((count/size)*100)

def groundTruth(path):
    ground_truth = defaultdict(set)

    #ground truth files have two different structures
    if(path != "youtube.top5000.txt" and path != "amazon.top5000.txt" ):
        with open(path) as file:
            for line in file.readlines():
                splitted = line.split()
                node_number = splitted[0]
                community_label = int(splitted[1])
            
            ground_truth[community_label].add(node_number)    
    else:
        count = 0
        with open(path) as file:
            for line in file.readlines():
                x = line.split('\t')
                for el in x:
                    el = el.rstrip()
                    ground_truth[count].add(el)
                count += 1
   
    return(ground_truth)

def main():
    gt_karateClub = "datasets/Karate/karate-club-labels.txt"
    gt_euCoreMail = "datasets/email-Eu-Core/email-Eu-core-department-labels.txt"
    gt_YouTube = "datasets/YouTube/youtube.top5000.txt"
    gt_Amazon = "datasets/Amazon/amazon.top5000.txt"
    gt_Football = "datasets/Football/football-comms.txt"

    edgeList_karateClub = "datasets/Karate/karate-club.txt"
    edgeList_euCoreMail = "datasets/email-Eu-Core/email-Eu-core.txt"
    edgeList_YouTube = "datasets/YouTube/youtube.txt"
    edgeList_Amazon = "datasets/Amazon/amazon.txt"
    edgeList_Football = "datasets/Football/football.txt"

    size_karate = 34
    size_football = 115
    size_euCore = 1005
    size_YouTube = 1157828
    size_Amazon = 334863
    
    #LabelPropagationAlgorithm(gt_karateClub, edgeList_karateClub, size_karate)
    #LabelPropagationAlgorithm(gt_Football, edgeList_Football, size_football)
    LabelPropagationAlgorithm(gt_euCoreMail, edgeList_euCoreMail, size_euCore)
    #LabelPropagationAlgorithm(gt_YouTube, edgeList_YouTube, size_YouTube)
    #LabelPropagationAlgorithm(gt_Amazon, edgeList_Amazon, size_Amazon)

    # GreedyModularityAlgorithm(gt_karateClub, edgeList_karateClub, size_karate)
    # GreedyModularityAlgorithm(gt_Football, edgeList_Football, size_football)
    GreedyModularityAlgorithm(gt_euCoreMail, edgeList_euCoreMail, size_euCore)
    # GreedyModularityAlgorithm(gt_YouTube, edgeList_YouTube, size_YouTube)
    # GreedyModularityAlgorithm(gt_Amazon, edgeList_Amazon, size_Amazon)

    # GirvanNewmanAlgorithm(gt_karateClub, edgeList_karateClub, size_karate)
    # GirvanNewmanAlgorithm(gt_Football, edgeList_Football, size_football)
    GirvanNewmanAlgorithm(gt_euCoreMail, edgeList_euCoreMail, size_euCore)
    # GirvanNewmanAlgorithm(gt_YouTube, edgeList_YouTube, size_YouTube)
    # GirvanNewmanAlgorithm(gt_Amazon, edgeList_Amazon, size_Amazon)
    
if __name__ == "__main__":
    #creates a blank file to results
    f = open("results.txt","w")
    f.close()
    main()

